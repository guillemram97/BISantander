{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Val, LGBM+XGB Model[LB - 1.40]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí es detalla el codi de https://www.kaggle.com/aditya1702/adversarial-val-lgbm-xgb-model-lb-1-40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guillem.ramirez\\Miniconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\guillem.ramirez\\Miniconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import gc\n",
    "import progressbar\n",
    "\n",
    "from scipy.stats import ks_2samp, skew, kurtosis\n",
    "from sklearn.model_selection import KFold, train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, scale\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, FastICA, FactorAnalysis, KernelPCA\n",
    "from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\", sep = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_drop_target(train, test):\n",
    "    y_train = train.target\n",
    "    y_train = np.log1p(y_train)\n",
    "    test_ID = test.index\n",
    "    train = train.drop(['target'])\n",
    "    return train, test, y_train, test_ID\n",
    "\n",
    "\n",
    "def remove_duplicate_columns(train, test): \n",
    "    train = train.T.drop_duplicates().T\n",
    "    columns_not_to_be_dropped = train.columns\n",
    "    columns_to_be_dropped = [col for col in test.columns if col not in columns_not_to_be_dropped]\n",
    "    test = test.drop(columns_to_be_dropped, 1)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def remove_constant_columns(train, test):\n",
    "    col_with_std_zero = train.loc[:, train.std(axis = 0) == 0].columns\n",
    "    train = train.loc[:, train.std(axis = 0) != 0]\n",
    "    test = test.drop(col_with_std_zero, 1) \n",
    "    return train, test\n",
    "\n",
    "\n",
    "def remove_features_using_importance(x_train, y_train, x_test):\n",
    "    def rmsle(actual, predicted):\n",
    "        return np.sqrt(np.mean(np.power(np.log1p(actual)-np.log1p(predicted), 2)))\n",
    "    num_of_features = 1000\n",
    "    x1, x2, y1, y2 = train_test_split(x_train, y_train, test_size = 0.20, random_state = 42)\n",
    "    model = RandomForestRegressor(n_jobs = -1, random_state = 7)\n",
    "    model.fit(x1, y1)\n",
    "    print(rmsle(np.expm1(y2), np.expm1(model.predict(x2)))) \n",
    "    col_df = pd.DataFrame({'importance': model.feature_importances_, 'feature': x_train.columns})\n",
    "    col_df_sorted = col_df.sort_values(by = ['importance'], ascending = [False])\n",
    "    columns = col_df_sorted[:num_of_features]['feature'].values   \n",
    "    x_train = x_train[columns]\n",
    "    x_test = x_test[columns]\n",
    "    return x_train, x_test\n",
    "\n",
    "def remove_features_having_different_distributions(train, test):\n",
    "    threshold_p_value = 0.01 \n",
    "    threshold_statistic = 0.3\n",
    "    cols_with_different_distributions = []\n",
    "    for col in train.columns:\n",
    "        #Kolmogorov-Smirnov test (comparar distribucions empíriques)\n",
    "        #això no és redundant? \n",
    "        statistic, pvalue = ks_2samp(train[col].values, test[col].values)\n",
    "        if pvalue <= threshold_p_value and np.abs(statistic) > threshold_statistic:\n",
    "            cols_with_different_distributions.append(col)\n",
    "    \n",
    "    for col in cols_with_different_distributions:\n",
    "        if col in train.columns:\n",
    "            train = train.drop(col, axis = 1)\n",
    "            test = test.drop(col, axis = 1)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara afegirem unes quantes columnes extres. La segona part és normalitzar les coses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_aggregate_features(train, test):\n",
    "    weight = ((train != 0).sum()/len(train)).values\n",
    "    tmp_train = train[train != 0]\n",
    "    tmp_test = test[test != 0]\n",
    "    print(\"weight count\")\n",
    "    train[\"weight_count\"] = (tmp_train * weight).sum(axis = 1)\n",
    "    test[\"weight_count\"] = (tmp_test * weight).sum(axis = 1)\n",
    "    \n",
    "    print(\"number of non-zero values\")\n",
    "    train[\"count_non_0\"] = (train != 0).sum(axis = 1)\n",
    "    test[\"count_non_0\"] = (test != 0).sum(axis = 1)\n",
    "    \n",
    "    print(\"number of different\")\n",
    "    train[\"num_different\"] = tmp_train.nunique(axis = 1)\n",
    "    test[\"num_different\"] = tmp_test.nunique(axis = 1)\n",
    "    \n",
    "    print(\"sum\")\n",
    "    train[\"sum\"] = train.sum(axis=1)\n",
    "    test[\"sum\"] = test.sum(axis=1)\n",
    "\n",
    "    print(\"variance\")\n",
    "    train[\"var\"] = tmp_train.var(axis=1)\n",
    "    test[\"var\"] = tmp_test.var(axis=1)\n",
    "\n",
    "    print(\"mean\")\n",
    "    train[\"mean\"] = tmp_train.mean(axis=1)\n",
    "    test[\"mean\"] = tmp_test.mean(axis=1)\n",
    "    \n",
    "    print(\"median\")\n",
    "    train[\"median\"] = tmp_train.median(axis=1)\n",
    "    test[\"median\"] = tmp_test.median(axis=1)\n",
    "\n",
    "    print(\"std\")\n",
    "    train[\"std\"] = tmp_train.std(axis=1)\n",
    "    test[\"std\"] = tmp_test.std(axis=1)\n",
    "\n",
    "    print(\"max\")\n",
    "    train[\"max\"] = tmp_train.max(axis=1)\n",
    "    test[\"max\"] = tmp_test.max(axis=1)\n",
    "\n",
    "    print(\"min\")\n",
    "    train[\"min\"] = tmp_train.min(axis=1)\n",
    "    test[\"min\"] = tmp_test.min(axis=1)\n",
    "    \n",
    "    print(\"skew\")\n",
    "    train[\"skew\"] = tmp_train.apply(skew, axis=1)\n",
    "    test[\"skew\"] = tmp_test.apply(skew, axis=1)\n",
    "    \n",
    "    print(\"kurtosis\")\n",
    "    train[\"kurtosis\"] = tmp_train.apply(kurtosis, axis=1)\n",
    "    test[\"kurtosis\"] = tmp_test.apply(kurtosis, axis=1)\n",
    "\n",
    "    \n",
    "    # Remove an NA valuess\n",
    "    train = train.fillna(0)\n",
    "    test = test.fillna(0)\n",
    "    \n",
    "    del(tmp_train)\n",
    "    del(tmp_test)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def mean_variance_scale_columns(total):\n",
    "    \n",
    "    p = progressbar.ProgressBar()\n",
    "    p.start()\n",
    "\n",
    "    # Mean-variance scale all columns excluding 0-values' \n",
    "    number_of_columns = len(total.columns)\n",
    "    for col_index, col in enumerate(total.columns):    \n",
    "        p.update(col_index/number_of_columns * 100)\n",
    "\n",
    "        # Detect outliers in this column\n",
    "        data = total[col].values\n",
    "        data_mean, data_std = np.mean(data), np.std(data)\n",
    "        cut_off = data_std * 3\n",
    "        lower, upper = data_mean - cut_off, data_mean + cut_off\n",
    "        outliers = [x for x in data if x < lower or x > upper]\n",
    "\n",
    "        # If there are crazy high values, do a log-transform\n",
    "        if len(outliers) > 0:\n",
    "            non_zero_idx = data != 0\n",
    "            total.loc[non_zero_idx, col] = np.log(data[non_zero_idx])\n",
    "\n",
    "        # Scale non-zero column values\n",
    "        nonzero_rows = (total[col] != 0)\n",
    "        if  np.isfinite(total.loc[nonzero_rows, col]).all():\n",
    "            total.loc[nonzero_rows, col] = scale(list(total.loc[nonzero_rows, col]))\n",
    "            if  np.isfinite(total[col]).all():\n",
    "                # Scale all column values\n",
    "                total[col] = scale(list(total[col]))\n",
    "        gc.collect()\n",
    "\n",
    "    p.finish()\n",
    "    \n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí utilitzarà PCA i altres tècniques de reducció de dimensionalitat. Obs: es poden fer servir tambe per ampliar-la!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ampliacio de dimensionalitat \n",
    "def add_decomposed_features_back_to_df(train, \n",
    "                                       test,\n",
    "                                       total,\n",
    "                                       n_components,\n",
    "                                       use_pca = False,\n",
    "                                       use_tsne = False,\n",
    "                                       use_tsvd = False,\n",
    "                                       use_ica = False,\n",
    "                                       use_fa = False,\n",
    "                                       use_grp = False,\n",
    "                                       use_srp = False):\n",
    "    \n",
    "    N_COMP = n_components\n",
    "    ntrain = len(train)\n",
    "    sparse_matrix = scipy.sparse.csr_matrix(total.values)\n",
    "    \n",
    "    if use_pca:\n",
    "        print(\"PCA\")\n",
    "        pca = PCA(n_components = N_COMP, random_state = 42)\n",
    "        pca_results = pca.fit_transform(total)\n",
    "        pca_results_train = pca_results[:ntrain]\n",
    "        pca_results_test = pca_results[ntrain:]\n",
    "        \n",
    "    if use_tsne:  \n",
    "        print(\"TSNE\")\n",
    "        tsne = TSNE(n_components = 3, init = 'pca')\n",
    "        tsne_results = tsne.fit_transform(total)\n",
    "        tsne_results_train = tsne_results[:ntrain]\n",
    "        tsne_results_test = tsne_results[ntrain:]\n",
    "\n",
    "    if use_tsvd:\n",
    "        print(\"tSVD\")\n",
    "        tsvd = TruncatedSVD(n_components = N_COMP, random_state = 42)\n",
    "        tsvd_results = tsvd.fit_transform(sparse_matrix)\n",
    "        tsvd_results_train = tsvd_results[:ntrain]\n",
    "        tsvd_results_test = tsvd_results[ntrain:]\n",
    "\n",
    "    if use_ica:\n",
    "        print(\"ICA\")\n",
    "        ica = FastICA(n_components = N_COMP, random_state=42)\n",
    "        ica_results = ica.fit_transform(total)\n",
    "        ica_results_train = ica_results[:ntrain]\n",
    "        ica_results_test = ica_results[ntrain:]\n",
    "\n",
    "    if use_fa:\n",
    "        print(\"FA\")\n",
    "        fa = FactorAnalysis(n_components = N_COMP, random_state=42)\n",
    "        fa_results = fa.fit_transform(total)\n",
    "        fa_results_train = fa_results[:ntrain]\n",
    "        fa_results_test = fa_results[ntrain:]\n",
    "\n",
    "    if use_grp:\n",
    "        print(\"GRP\")\n",
    "        grp = GaussianRandomProjection(n_components = N_COMP, eps = 0.1, random_state = 42)\n",
    "        grp_results = grp.fit_transform(total)\n",
    "        grp_results_train = grp_results[:ntrain]\n",
    "        grp_results_test = grp_results[ntrain:]\n",
    "\n",
    "    if use_srp:\n",
    "        print(\"SRP\")\n",
    "        srp = SparseRandomProjection(n_components = N_COMP, dense_output=True, random_state=42)\n",
    "        srp_results = srp.fit_transform(total)\n",
    "        srp_results_train = srp_results[:ntrain]\n",
    "        srp_results_test = srp_results[ntrain:]\n",
    "\n",
    "    print(\"Append decomposition components to datasets...\")\n",
    "    for i in range(1, N_COMP + 1):\n",
    "        \n",
    "        if use_pca:\n",
    "            train['pca_' + str(i)] = pca_results_train[:, i - 1]\n",
    "            test['pca_' + str(i)] = pca_results_test[:, i - 1]\n",
    "            \n",
    "        if use_tsne:\n",
    "            train['tsne_' + str(i)] = tsne_results_train[:, i - 1]\n",
    "            test['tsne_' + str(i)] = tsne_results_test[:, i - 1]\n",
    "            \n",
    "        if use_tsvd:\n",
    "            train['tsvd_' + str(i)] = tsvd_results_train[:, i - 1]\n",
    "            test['tsvd_' + str(i)] = tsvd_results_test[:, i - 1]\n",
    "        \n",
    "        if use_ica:\n",
    "            train['ica_' + str(i)] = ica_results_train[:, i - 1]\n",
    "            test['ica_' + str(i)] = ica_results_test[:, i - 1]\n",
    "        \n",
    "        if use_fa:\n",
    "            train['fa_' + str(i)] = fa_results_train[:, i - 1]\n",
    "            test['fa_' + str(i)] = fa_results_test[:, i - 1]\n",
    "\n",
    "        if use_grp:\n",
    "            train['grp_' + str(i)] = grp_results_train[:, i - 1]\n",
    "            test['grp_' + str(i)] = grp_results_test[:, i - 1]\n",
    "        \n",
    "        if use_srp:\n",
    "            train['srp_' + str(i)] = srp_results_train[:, i - 1]\n",
    "            test['srp_' + str(i)] = srp_results_test[:, i - 1]\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "\n",
    "def use_decomposed_features_as_new_df(train, \n",
    "                                      test,\n",
    "                                      total,\n",
    "                                      n_components,\n",
    "                                      use_pca = False,\n",
    "                                      use_tsvd = False,\n",
    "                                      use_ica = False,\n",
    "                                      use_fa = False,\n",
    "                                      use_grp = False,\n",
    "                                      use_srp = False):\n",
    "    N_COMP = n_components\n",
    "    ntrain = len(train)\n",
    "\n",
    "    if use_pca:\n",
    "        print(\"PCA\")\n",
    "        pca = PCA(n_components = N_COMP, random_state = 42)\n",
    "        pca_results = pca.fit_transform(total)\n",
    "        pca_results_train = pca_results[:ntrain]\n",
    "        pca_results_test = pca_results[ntrain:]\n",
    "\n",
    "    if use_tsvd:\n",
    "        print(\"tSVD\")\n",
    "        tsvd = TruncatedSVD(n_components = N_COMP, random_state=42)\n",
    "        tsvd_results = tsvd.fit_transform(total)\n",
    "        tsvd_results_train = tsvd_results[:ntrain]\n",
    "        tsvd_results_test = tsvd_results[ntrain:]\n",
    "\n",
    "    if use_ica:\n",
    "        print(\"ICA\")\n",
    "        ica = FastICA(n_components = N_COMP, random_state=42)\n",
    "        ica_results = ica.fit_transform(total)\n",
    "        ica_results_train = ica_results[:ntrain]\n",
    "        ica_results_test = ica_results[ntrain:]\n",
    "\n",
    "    if use_fa:\n",
    "        print(\"FA\")\n",
    "        fa = FactorAnalysis(n_components = N_COMP, random_state=42)\n",
    "        fa_results = fa.fit_transform(total)\n",
    "        fa_results_train = fa_results[:ntrain]\n",
    "        fa_results_test = fa_results[ntrain:]\n",
    "\n",
    "    if use_grp:\n",
    "        print(\"GRP\")\n",
    "        grp = GaussianRandomProjection(n_components = N_COMP, eps=0.1, random_state=42)\n",
    "        grp_results = grp.fit_transform(total)\n",
    "        grp_results_train = grp_results[:ntrain]\n",
    "        grp_results_test = grp_results[ntrain:]\n",
    "\n",
    "    if use_srp:\n",
    "        print(\"SRP\")\n",
    "        srp = SparseRandomProjection(n_components = N_COMP, dense_output=True, random_state=42)\n",
    "        srp_results = srp.fit_transform(total)\n",
    "        srp_results_train = srp_results[:ntrain]\n",
    "        srp_results_test = srp_results[ntrain:]\n",
    "        \n",
    "    print(\"Append decomposition components together...\")\n",
    "    train_decomposed = np.concatenate([srp_results_train, grp_results_train, ica_results_train, pca_results_train, tsvd_results_train], axis=1)\n",
    "    test_decomposed = np.concatenate([srp_results_test, grp_results_test, ica_results_test, pca_results_test, tsvd_results_test], axis=1)\n",
    "\n",
    "    train_with_only_decomposed_features = pd.DataFrame(train_decomposed)\n",
    "    test_with_only_decomposed_features = pd.DataFrame(test_decomposed)\n",
    "    \n",
    "    # typo, no?\n",
    "    for agg_col in ['sum', 'var', 'mean', 'median', 'std', 'weight_count', 'count_non_0', 'num_different', 'max', 'min']:\n",
    "        train_with_only_decomposed_features[col] = train[col]\n",
    "        test_with_only_decomposed_features[col] = test[col]\n",
    "    \n",
    "    # Remove any NA\n",
    "    train_with_only_decomposed_features = train_with_only_decomposed_features.fillna(0)\n",
    "    test_with_only_decomposed_features = test_with_only_decomposed_features.fillna(0)\n",
    "    \n",
    "    return train_with_only_decomposed_features, test_with_only_decomposed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 67111122668563.78\n"
     ]
    }
   ],
   "source": [
    "def generate_adversarial_validation_set(train, test):\n",
    "    x_test = test.drop([\"is_test\", \"target\"], 1)\n",
    "\n",
    "    train, val = train[train.predicted_probs < 0.9], train[train.predicted_probs > 0.9]\n",
    "    train = train.drop([\"is_test\", \"predicted_probs\"], 1)\n",
    "    val = val.drop([\"is_test\", \"predicted_probs\"], 1)\n",
    "\n",
    "    x_train, y_train = train.drop(\"target\", 1), train.target\n",
    "    x_val, y_val = val.drop(\"target\", 1), val.target\n",
    "    \n",
    "    return x_train, y_train, x_val, y_val, x_test\n",
    "\n",
    "def get_training_set_with_test_set_similarity_predictions(X_train, Y_train, X_test):\n",
    "    X_train['target'] = Y_train\n",
    "    X_test['target'] = 0\n",
    "    \n",
    "    X_train[\"is_test\"] = 0\n",
    "    X_test[\"is_test\"] = 1\n",
    "    assert(np.all(train.columns == test.columns))\n",
    "    \n",
    "    print(\"Concat train and test data\")\n",
    "    total = pd.concat([X_train, X_test])\n",
    "    total = total.fillna(0)\n",
    "    \n",
    "    x = total.drop([\"is_test\", \"target\"], axis = 1)\n",
    "    y = total.is_test\n",
    "    \n",
    "    print(\"Start cross-validating\")\n",
    "    n_estimators = 100\n",
    "    classifier = RandomForestClassifier(n_estimators = n_estimators, n_jobs = -1)\n",
    "    predictions = np.zeros(y.shape)\n",
    "    \n",
    "    stratified_kfold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 5678)\n",
    "    \n",
    "    for fold_index, (train_indices, test_indices) in enumerate(stratified_kfold.split(x, y)):\n",
    "        print(\"Fold - \" + str(fold_index))\n",
    "        \n",
    "        x_train = x.iloc[train_indices]\n",
    "        y_train = y.iloc[train_indices]\n",
    "        x_test = x.iloc[test_indices]\n",
    "        y_test = y.iloc[test_indices]\n",
    "        \n",
    "        classifier.fit(x_train, y_train)\n",
    "        \n",
    "        predicted_probabilities = classifier.predict_proba(x_test)[:, 1]\n",
    "        \n",
    "        auc = roc_auc_score(y_test, predicted_probabilities)\n",
    "        print(\"AUC Score - \" + str(auc) + \"%\")\n",
    "        \n",
    "        predictions[test_indices] = predicted_probabilities\n",
    "    total['predicted_probs'] = predictions\n",
    "    \n",
    "    print(\"Generating training set\")\n",
    "    total = total[total.is_test == 0]\n",
    "    \n",
    "    print(\"Sorting according to predictions\")\n",
    "    train_set_with_predictions_for_test_set_similarity = total.sort_values([\"predicted_probs\"], ascending = False)\n",
    "    \n",
    "    return train_set_with_predictions_for_test_set_similarity, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSLE(actual, predicted):\n",
    "    return np.sqrt(np.mean(np.power(np.log1p(actual) - np.log1p(predicted), 2)))\n",
    "\n",
    "# LightGBM Model\n",
    "def run_lgb(x_train, y_train, x_val, y_val, x_test):\n",
    "    print(\"############# Build LightGBM model #############\")\n",
    "    model_lgb = lgb.LGBMRegressor(objective='regression',\n",
    "                                  num_leaves = 144,\n",
    "                                  learning_rate = 0.005, \n",
    "                                  n_estimators = 720, \n",
    "                                  max_depth = 13,\n",
    "                                  metric = 'rmse',\n",
    "                                  is_training_metric = True,\n",
    "                                  max_bin = 55, \n",
    "                                  bagging_fraction = 0.8,\n",
    "                                  verbose = -1,\n",
    "                                  bagging_freq = 5, \n",
    "                                  feature_fraction = 0.9)\n",
    "    \n",
    "    print(\"Validating.....\")\n",
    "    model_lgb.fit(x_train, y_train, eval_set = (x_val, y_val), early_stopping_rounds = 100, verbose = True, eval_metric = 'rmse')\n",
    "    \n",
    "    print(\"Train on full dataset\")\n",
    "    X_TRAIN = pd.concat([x_train, x_val])\n",
    "    Y_TRAIN = pd.concat([y_train, y_val])\n",
    "    \n",
    "    model_lgb.fit(X = X_TRAIN,\n",
    "                  y = Y_TRAIN)\n",
    "    \n",
    "    \n",
    "    y_pred_test = np.expm1(model_lgb.predict(X_TEST))\n",
    "    print(\"LightGBM Training Completed...\")\n",
    "    \n",
    "    return y_pred_test\n",
    "\n",
    "# XGBoost Model\n",
    "def run_xgb(x_train, y_train, x_val, y_val, X_TEST):\n",
    "    print(\"############# Build XGBoost model #############\")\n",
    "    model_xgb = xgb.XGBRegressor(colsample_bytree = 0.055, \n",
    "                                 colsample_bylevel = 0.5, \n",
    "                                 gamma = 1.5, \n",
    "                                 learning_rate = 0.02, \n",
    "                                 max_depth = 32, \n",
    "                                 objective = 'reg:linear',\n",
    "                                 booster = 'gbtree',\n",
    "                                 min_child_weight = 57, \n",
    "                                 n_estimators = 1000, \n",
    "                                 reg_alpha = 0, \n",
    "                                 reg_lambda = 0,\n",
    "                                 eval_metric = 'rmse', \n",
    "                                 subsample = 0.7, \n",
    "                                 silent = 1, \n",
    "                                 n_jobs = -1, \n",
    "                                 early_stopping_rounds = 14,\n",
    "                                 random_state = 7, \n",
    "                                 nthread = -1)\n",
    "    \n",
    "    print(\"Validating.....\")\n",
    "    model_xgb.fit(x_train, y_train, eval_set = [(x_train, y_train), (x_val, y_val)], eval_metric = 'rmse', early_stopping_rounds = 100, verbose = True)\n",
    "    \n",
    "    print(\"Train on full dataset\")\n",
    "    X_TRAIN = pd.concat([x_train, x_val])\n",
    "    Y_TRAIN = pd.concat([y_train, y_val])\n",
    "    \n",
    "    model_xgb.fit(X = X_TRAIN, \n",
    "                  y = Y_TRAIN)\n",
    "    \n",
    "    y_pred_test = np.expm1(model_xgb.predict(X_TEST))\n",
    "    print(\"XGBoost Training Completed...\")\n",
    "    \n",
    "    return y_pred_test\n",
    "    \n",
    "# CatBoost Model\n",
    "# def run_cbm(x_train, y_train, x_val, y_val, X_TEST):\n",
    "#     print(\"############# Build CatBoost model #############\")\n",
    "    \n",
    "#     model_cb = cb.CatBoostRegressor(iterations = 500,\n",
    "#                                  learning_rate = 0.05,\n",
    "#                                  depth = 10,\n",
    "#                                  eval_metric = 'RMSE',\n",
    "#                                  random_seed = 42,\n",
    "#                                  bagging_temperature = 0.2,\n",
    "#                                  od_type = 'Iter',\n",
    "#                                  metric_period = 50,\n",
    "#                                  od_wait = 20)\n",
    "    \n",
    "#     model_cb.fit(x_train, y_train)\n",
    "#     rmsle = RMSLE(np.expm1(y_val), np.expm1(model_cb.predict(x_val)))\n",
    "#     print(\"The RMSLE score on validation set is - \" + str(rmsle))\n",
    "    \n",
    "#     X_TRAIN = pd.concat([x_train, x_val])\n",
    "#     Y_TRAIN = pd.concat([y_train, y_val])\n",
    "    \n",
    "#     model_cb.fit(X = X_TRAIN, \n",
    "#                  y = Y_TRAIN)\n",
    "    \n",
    "#     y_pred_test = np.expm1(model_cb.predict(X_TEST))\n",
    "#     print(\"CatBoost Training Completed...\")\n",
    "    \n",
    "#     return y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predictions(lgb_pred = None, xgb_pred = None, cbm_pred = None, lgb_ratio = 1/3, xgb_ratio = 1/3, cbm_ratio = 1/3):\n",
    "    print(\"############# Ensemble model predictions #############\")\n",
    "    \n",
    "    y_pred_test_final = lgb_pred * lgb_ratio + xgb_pred * xgb_ratio #+ cbm_pred * cbm_ratio\n",
    "    return y_pred_test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_submission_file(y_pred_test, test_ID, model_name):\n",
    "    sub = pd.DataFrame(y_pred_test)\n",
    "    sub.columns = ['target']\n",
    "    sub.insert(0, 'ID', test_ID)\n",
    "    print(sub.head())\n",
    "    sub.to_csv(model_name +'_10_all_decomposition_features.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executar-ho tot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_orig, test_orig = read_data()\n",
    "####################################\n",
    "####################################\n",
    "train, test, y_train, test_ID = convert_and_drop_target(train = train_orig.copy(), \n",
    "                                                        test = test_orig.copy())\n",
    "\n",
    "train_without_duplicate_columns, test_without_duplicate_columns = remove_duplicate_columns(train = train.copy(), \n",
    "                                                                                           test = test.copy())\n",
    "\n",
    "train_without_constant_columns, test_without_constant_columns = remove_constant_columns(train = train_without_duplicate_columns.copy(), \n",
    "                                                                                        test = test_without_duplicate_columns.copy())\n",
    "\n",
    "train_with_columns_of_high_importance, test_with_columns_of_high_importance = remove_features_using_importance(x_train = train_without_constant_columns.copy(), \n",
    "                                                                                                              y_train = y_train.copy(), \n",
    "                                                                                                              x_test = test_without_constant_columns.copy())\n",
    "\n",
    "train_with_columns_having_same_distributions, test_with_columns_having_same_distributions = remove_features_having_different_distributions(train = train_with_columns_of_high_importance.copy(), \n",
    "                                                                                                                                           test = test_with_columns_of_high_importance.copy())\n",
    "\n",
    "total = pd.concat([train_with_columns_having_same_distributions, test_with_columns_having_same_distributions])\n",
    "total_scaled = mean_variance_scale_columns(total = total.copy())\n",
    "\n",
    "train_with_aggregate_features, test_with_aggregate_features = add_aggregate_features(train = train_with_columns_having_same_distributions.copy(), \n",
    "                                                                                     test = test_with_columns_having_same_distributions.copy())\n",
    "\n",
    "\n",
    "train_with_decomposed_features, test_with_decomposed_features = add_decomposed_features_back_to_df(train = train_with_aggregate_features.copy(),\n",
    "                                                                                                 test = test_with_aggregate_features.copy(),\n",
    "                                                                                                 total = total_scaled.copy(),\n",
    "                                                                                                 n_components = 100,\n",
    "                                                                                                 use_pca = False,\n",
    "                                                                                                 use_tsne = False,\n",
    "                                                                                                 use_tsvd = False,\n",
    "                                                                                                 use_ica = False,\n",
    "                                                                                                 use_fa = False,\n",
    "                                                                                                 use_grp = False,\n",
    "                                                                                                 use_srp = True)\n",
    "\n",
    "\n",
    "training_set, testing_set = get_training_set_with_test_set_similarity_predictions(train_with_decomposed_features.copy(), y_train.copy(), test_with_decomposed_features.copy())\n",
    "\n",
    "x_train, y_train, x_val, y_val, X_TEST = generate_adversarial_validation_set(training_set.copy(), testing_set.copy())\n",
    "\n",
    "###################################\n",
    "###################################\n",
    "# y_pred_test_lbg = run_lgb(x_train, y_train, x_val, y_val, X_TEST)\n",
    "y_pred_test_xgb = run_xgb(x_train, y_train, x_val, y_val, X_TEST)\n",
    "# y_pred_test_cbm = run_cbm(x_train, y_train, x_val, y_val, X_TEST)\n",
    "\n",
    "y_pred_test_final = ensemble_predictions(lgb_pred = y_pred_test_lbg, \n",
    "                                         xgb_pred = y_pred_test_xgb, \n",
    "                                         cbm_pred = None, \n",
    "                                         lgb_ratio = 1/3, \n",
    "                                         xgb_ratio = 1/3, \n",
    "                                         cbm_ratio = 1/3)\n",
    "\n",
    "save_submission_file(y_pred_test = y_pred_test_final, test_ID = test_ID, model_name = \"Ensemble_LGB_XGB_with_columns_scaled\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
